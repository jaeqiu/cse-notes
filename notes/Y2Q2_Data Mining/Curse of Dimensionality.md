1. In [Machine Learning](../Y2Q1_Machine%20Learning/Machine%20Learning.canvas), This refers to how having more features is not always better. Of course, more features allows for more dimensions for any object to by different from any other object, which would, at first sight, be productive for devising an accurate machine learning model, however, the amount of necessary training objects increases at a fast rate, leading to a certain tradeoff being optimal in the long run.![](../../attachments/image/Curse%20of%20Dimensionality-1737889488937.jpeg)![](../../attachments/image/Curse%20of%20Dimensionality-1737889517682.jpeg)
3. In [Data Mining](Data%20Mining.md), This refers to how [distances](Distances.md) start to lose intuitive meaning, when you inflate to high dimensions. All distances start to become more similar. When increasing the amount of dimensions, after the point at which every inherent difference (of the data) is captured, adding more dimensions will only "concentrate" objects. (The additional dimensions will not capture any differences, only do nothing or concentrate multiple objects to the same range)