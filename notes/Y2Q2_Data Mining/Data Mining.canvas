{
	"nodes":[
		{"id":"5127864e4a7945e0","type":"link","url":"https://peer.tudelft.nl/courses","x":-1200,"y":-3350,"width":800,"height":734},
		{"id":"f4d91e11f8f46756","type":"link","url":"https://answers.ewi.tudelft.nl/","x":-320,"y":-3350,"width":800,"height":734},
		{"id":"cf2ea9039af26bb2","type":"file","file":"pdf/dm_book2.pdf","x":560,"y":-3350,"width":800,"height":734},
		{"id":"f366e44bfef7a462","type":"link","url":"https://brightspace.tudelft.nl/d2l/le/content/680741/Home","x":1440,"y":-3350,"width":800,"height":734},
		{"id":"7f130966584c47f6","type":"file","file":"pdf/dm_book.pdf","x":2320,"y":-3350,"width":800,"height":734},
		{"id":"99b6dead42c7b6f0","type":"file","file":"notes/Y2Q2_Data Mining/Cosine similarity.md","x":5040,"y":-6560,"width":800,"height":734},
		{"id":"1d2915e7c0033a27","type":"file","file":"pdf/DM2024_lecture1_Introduction.pdf","x":5040,"y":-5610,"width":800,"height":734},
		{"id":"dd3a93c066741929","type":"text","text":"Manifold learning\n- Manifolds\n  t-Disttributed Stochastic Neighbor Embedding (t-SNE)\n  - Uniform Manifold AApproximation and Projection (UMA)\n- DImensionality reduction: When is PCA not enough?","x":5120,"y":-4500,"width":800,"height":734},
		{"id":"fb63a8b0cd20e223","type":"text","text":"Cross validation for recommender systems\nlarge data -> leave on out \nSpaprse data  -> k-fold with small k \n","x":4240,"y":-4500,"width":800,"height":734},
		{"id":"d6074c54ce077a12","type":"link","url":"https://weblab.tudelft.nl/","x":3280,"y":-6560,"width":800,"height":734},
		{"id":"71978c8d20ceb704","type":"file","file":"notes/Y2Q2_Data Mining/Data Mining.md","x":4160,"y":-6560,"width":800,"height":734},
		{"id":"a8026b68cca84ea5","type":"file","file":"notes/Y2Q2_Data Mining/NOT ON EXAM.md","x":2400,"y":-6560,"width":800,"height":734},
		{"id":"5c383bdcb82e0faa","type":"file","file":"png/Pasted image 20241118144734.png","x":2400,"y":-5610,"width":729,"height":734},
		{"id":"3faa3b7cc7a42ce2","type":"text","text":"# Cross entropy\n\n![[png/Pasted image 20241118150757.png]]","x":3280,"y":-5610,"width":800,"height":734},
		{"id":"d64d0d7c8016e259","type":"file","file":"notes/Y2Q2_Data Mining/Distance Metric.md","x":4160,"y":-5610,"width":800,"height":734},
		{"id":"a6a9566eb9948406","type":"link","url":"https://queue.tudelft.nl/","x":1520,"y":-6560,"width":800,"height":734},
		{"id":"743d697f09d124fe","type":"text","text":"# Heterogeneous data types\n\nNumerical values\nCategorical data\ntext","x":1520,"y":-5610,"width":800,"height":734},
		{"id":"61d149e11a923374","type":"file","file":"notes/Y2Q2_Data Mining/KL-divergence measure.md","x":640,"y":-5610,"width":800,"height":734},
		{"id":"94d21923cd9f50c3","type":"text","text":"Redlining effect:\n\nremoving the sensitive feature does not remove the ability to discriminate => other attributes can be correlated and therefore infer them.","x":720,"y":-4500,"width":800,"height":734},
		{"id":"60f05963dba5701b","type":"link","url":"https://www.kdnuggets.com/","x":1600,"y":-4500,"width":800,"height":734},
		{"id":"769da6e3f6455734","type":"file","file":"png/Pasted image 20241216224525.png","x":2480,"y":-4500,"width":800,"height":492},
		{"id":"1b1f6396f89c969c","type":"text","text":"SPearman Correlation vs Pearson Correlation\n\nspearman's rho is used for measuring the monotonocity of two variables, while the pearsson correlation will mearsure only the linear correlations.\n\n","x":3360,"y":-4500,"width":800,"height":734},
		{"id":"5a7918f5086a14d7","type":"link","url":"https://peer.tudelft.nl/courses","x":-1120,"y":-6560,"width":800,"height":734},
		{"id":"db293a934e8563cf","type":"link","url":"https://mattermost.tudelft.nl/select_team","x":-240,"y":-6560,"width":800,"height":734},
		{"id":"b3665979e1f47b3a","type":"link","url":"https://www.kaggle.com/","x":640,"y":-6560,"width":800,"height":734},
		{"id":"1375529670ba679f","type":"text","text":"small /#dimensions -> use euclidean distance\n\nhuge amount of dimensions => use e.g. cosine","x":-1120,"y":-5610,"width":800,"height":734},
		{"id":"0ae807517dc89449","type":"file","file":"notes/Y2Q2_Data Mining/Jaccard Similarity Distance.md","x":-240,"y":-5610,"width":800,"height":734},
		{"id":"676912ea3dce1a91","type":"text","text":"Prosecutor's fallacy:\nP(A|B) != P(B|A)\n","x":-1040,"y":-4500,"width":800,"height":734},
		{"id":"5638eb86c506b802","type":"text","text":"Data dredging:\nYou will expect to find correlations (From any data set)\nyou will find what you want to find if you try hard enough\nthe data will confess to anything if tortured long enough\n\nAVOID USING CROSS VALIDATION (FIND OUT MEANINGLESSNESS)\n\nhow tf we gon do cross validation if we have no ground truth","x":-160,"y":-4500,"width":800,"height":734}
	],
	"edges":[]
}